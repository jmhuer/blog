
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Defenitions &#8212; Juan</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Datasets" href="datasets.html" />
    <link rel="prev" title="Computer Vision" href="index.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/0.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Juan</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="https://jmhuer.github.io">
   ← jmhuer.github.io
    
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../natural-language-processing/index.html">
   Natural Language Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cloud/index.html">
   Cloud
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="index.html">
   Computer Vision
  </a>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Defenitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="datasets.html">
     Datasets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="../audio-processing/index.html">
   Audio Processing
  </a>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Defenitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="datasets.html">
     Datasets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../generative-models/index.html">
   Generative Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../optimization/index.html">
   Optimization
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
   
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/computer-vision/topics&tools.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-classification">
   Image classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nlp">
   NLP
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-localization">
   Image localization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#coco">
     COCO
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="defenitions">
<h1>Defenitions<a class="headerlink" href="#defenitions" title="Permalink to this headline">¶</a></h1>
<p>In machine learning and deep learning we can’t do anything without data. So the people that create datasets for us to train our models are the (often under-appreciated) heros. Some of the most useful and important datasets are those that become important “academic baselines”; that is, datasets that are widely studied by researchers and used to compare algorithmic changes. Some of these become household names (at least, among households that train models!), such as <em>MNIST</em>, <em>CIFAR 10</em>, and <em>Imagenet</em>.</p>
<p>At fast.ai we (and our students) owe a debt of gratitude to those kind folks who have made datasets available for the research community. We’ve teamed up with AWS to try to give back a little: we’ve made some of the most important of these datasets available in a single place, using standard formats, on reliable and fast infrastructure (see below for a full list and links). If you use any of these datasets in your research, please give back by citing the original paper (we’ve provided the appropriate citation link below for each), and if you use them as part of a commercial or educational project, consider adding a note of thanks and a link to the dataset.</p>
<p>We use these datasets in our teaching, because they provide great examples of the kind of data that students are likely to encounter, and the academic literature has many examples of model results using these datasets which students can compare their work to. In addition, we also use datasets from <a class="reference external" href="http://www.kaggle.com">Kaggle Competitions</a>, because the public leaderboards on Kaggle allow students to test their models against the best in the world (the Kaggle datasets are not listed here).</p>
<p>For each dataset below, click the ‘source’ link to see the dataset license and details from the creator, the ‘cite’ link for the paper for citations, and the ‘download’ link to access to dataset from <a class="reference external" href="https://registry.opendata.aws/">AWS Open Datasets</a>.</p>
<div class="section" id="image-classification">
<h2><a class="reference external" href="https://registry.opendata.aws/fast-ai-imageclas/">Image classification</a><a class="headerlink" href="#image-classification" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>Citation</p></th>
<th class="head"><p>Download</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST</a></p></td>
<td><p><a class="reference external" href="http://yann.lecun.com/exdb/publis/index.html#lecun-98">LeCun et al., 1998a</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz">download</a></p></td>
<td><p>Classic dataset of small (28x28) handwritten grayscale digits, developed in the 1990s for testing the most sophisticated models of the day; today, often used as a basic “hello world” for introducing deep learning. This fast.ai datasets version uses a standard PNG format instead of the special binary format of the original, so you can use the regular data pipelines in most libraries; if you want to use just a single input channel like the original, simply pick a single slice from the channels axis.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a></p></td>
<td><p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">Krizhevsky, 2009</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz">download</a></p></td>
<td><p>60000 32x32 colour images in 10 classes, with 6000 images per class (50000 training images and 10000 test images). Very widely used today for testing performance of new algorithms. This fast.ai datasets version uses a standard PNG format instead of the platform-specific binary formats of the original, so you can use the regular data pipelines in most libraries.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR100</a></p></td>
<td><p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">Krizhevsky, 2009</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/cifar100.tgz">download</a></p></td>
<td><p>This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a “fine” label (the class to which it belongs) and a “coarse” label (the superclass to which it belongs).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">Caltech-UCSD Birds-200-2011</a></p></td>
<td><p><a class="reference external" href="http://vis-www.cs.umass.edu/bcnn/">Lin et al. 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz">download</a></p></td>
<td><p>An image dataset with photos of 200 bird species (mostly North American); it can also be used for localization. Number of categories: 200; Number of images: 11,788; Annotations per image: 15 Part Locations, 312 Binary Attributes, 1 Bounding Box</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">Caltech 101</a></p></td>
<td><p><a class="reference external" href="http://www.vision.caltech.edu/feifeili/Fei-Fei_GMBV04.pdf">L. Fei-Fei et al., 2004</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/caltech_101.tar.gz">download</a></p></td>
<td><p>Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. The size of each image is roughly 300 x 200 pixels. Can also be used for localization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://www.robots.ox.ac.uk/~vgg/data/pets/">Oxford-IIIT Pet</a></p></td>
<td><p><a class="reference external" href="http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf">O. M. Parkhi et al., 2012</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz">download</a></p></td>
<td><p>A 37 category pet dataset with roughly 200 images for each class. The images have a large variations in scale, pose and lighting. Can also be used for localization.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://www.robots.ox.ac.uk/~vgg/data/flowers/102/">Oxford 102 Flowers</a></p></td>
<td><p><a class="reference external" href="http://www.robots.ox.ac.uk/~vgg/publications/papers/nilsback08.pdf">Nilsback, M-E. and Zisserman, A., 2008</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz">download</a></p></td>
<td><p>A 102 category dataset consisting of 102 flower categories, commonly occuring in the United Kingdom. Each class consists of 40 to 258 images. The images have large scale, pose and light variations.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.vision.ee.ethz.ch/datasets_extra/food-101/">Food-101</a></p></td>
<td><p><a class="reference external" href="https://pdfs.semanticscholar.org/8e3f/12804882b60ad5f59aad92755c5edb34860e.pdf">Bossard, Lukas et al., 2014</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/food-101.tgz">download</a></p></td>
<td><p>101 food categories, with 101,000 images; 250 test images and 750 training images per class. The training images were not cleaned. All images were rescaled to have a maximum side length of 512 pixels.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://ai.stanford.edu/~jkrause/cars/car_dataset.html">Stanford cars</a></p></td>
<td><p><a class="reference external" href="https://ai.stanford.edu/~jkrause/papers/3drr13.pdf">Jonathan Krause et al., 2013</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/stanford-cars.tgz">download</a></p></td>
<td><p>16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/fastai/imagenette">Imagenette</a></p></td>
<td><p>Based on <a class="reference external" href="http://www.image-net.org/papers/imagenet_cvpr09.bib">Deng et al., 2009</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/imagenette.tgz">Full size</a> <a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz">320 px</a> <a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160.tgz">160 px</a></p></td>
<td><p>A subset of 10 easily classified classes from Imagenet: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/fastai/imagenette">Imagewoof</a></p></td>
<td><p>Based on <a class="reference external" href="http://www.image-net.org/papers/imagenet_cvpr09.bib">Deng et al., 2009</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/imagewoof.tgz">Full size</a> <a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-320.tgz">320 px</a> <a class="reference external" href="https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-160.tgz">160 px</a></p></td>
<td><p>A subset of 10 harder to classify classes from Imagenet (all dog breeds): Australian terrier, Border terrier, Samoyed, beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, dingo, golden retriever, Old English sheepdog</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="nlp">
<h2><a class="reference external" href="https://registry.opendata.aws/fast-ai-nlp/">NLP</a><a class="headerlink" href="#nlp" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>Citation</p></th>
<th class="head"><p>Download</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="http://ai.stanford.edu/~amaas/data/sentiment/">IMDb Large Movie Review Dataset</a></p></td>
<td><p><a class="reference external" href="http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf">Andrew L. Maas et al., 2011</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/imdb.tgz">download</a></p></td>
<td><p>A dataset for binary sentiment classification containing 25,000 highly polarized movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">Wikitext-103</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1609.07843">Stephen Merity et al., 2016</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/wikitext-103.tgz">download</a></p></td>
<td><p>A collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. Widely used for language modeling, including the pretrained models used in the fastai library and ULMFiT algorithm.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">Wikitext-2</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1609.07843">Stephen Merity et al., 2016</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz">download</a></p></td>
<td><p>A subset of Wikitext-103; useful for testing language model training on smaller datasets.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://www.statmt.org/wmt15/translation-task.html">WMT 2015 French/English parallel texts</a></p></td>
<td><p><a class="reference external" href="https://www.cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf">Callison-Burch et al., 2009</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz">download</a></p></td>
<td><p>French/English parallel texts for training translation models. Over 20 million sentences in French and English. Dataset created by Chris Callison-Burch, who crawled millions of web pages and then used a set of simple heuristics to transform French URLs onto English URLs, and assumed that these documents are translations of each other.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://xzh.me/docs/charconvnet.pdf">AG News</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1509.01626">Xiang Zhang et al., 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz">download</a></p></td>
<td><p>496,835 categorized news articles from &gt;2000 news sources from the 4 largest classes from AG’s corpus of news articles, using only the title and description fields. The number of training samples for each class is 30,000 and testing 1900.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://xzh.me/docs/charconvnet.pdf">Amazon reviews - Full</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1509.01626">Xiang Zhang et al., 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/amazon_review_full_csv.tgz">download</a></p></td>
<td><p>34,686,770 Amazon reviews from 6,643,669 users on 2,441,053 products, from the Stanford Network Analysis Project (SNAP).  This full dataset contains 600,000 training samples and 130,000 testing samples in each class.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://xzh.me/docs/charconvnet.pdf">Amazon reviews - Polarity</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1509.01626">Xiang Zhang et al., 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz">download</a></p></td>
<td><p>34,686,770 Amazon reviews from 6,643,669 users on 2,441,053 products, from the Stanford Network Analysis Project (SNAP).  This subset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://xzh.me/docs/charconvnet.pdf">DBPedia ontology</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1509.01626">Xiang Zhang et al., 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/dbpedia_csv.tgz">download</a></p></td>
<td><p>40,000 training samples and 5,000 testing samples from 14 nonoverlapping classes from DBpedia 2014.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://xzh.me/docs/charconvnet.pdf">Sogou news</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1509.01626">Xiang Zhang et al., 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/sogou_news_csv.tgz">download</a></p></td>
<td><p>2,909,551 news articles from the SogouCA and SogouCS news corpora, in 5 categories. The number of training samples selected for each class is 90,000 and testing 12,000. Note that the Chinese characters have been converted to Pinyin.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://xzh.me/docs/charconvnet.pdf">Yahoo! Answers</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1509.01626">Xiang Zhang et al., 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/yahoo_answers_csv.tgz">download</a></p></td>
<td><p>The  10 largest main categories from the Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset. Each class contains 140,000 training samples and 5,000 testing samples.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://xzh.me/docs/charconvnet.pdf">Yelp reviews - Full</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1509.01626">Xiang Zhang et al., 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/yelp_review_full_csv.tgz">download</a></p></td>
<td><p>1,569,264 samples from the Yelp Dataset Challenge 2015. This full dataset has 130,000 training samples and 10,000 testing samples in each star.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://xzh.me/docs/charconvnet.pdf">Yelp reviews - Polarity</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1509.01626">Xiang Zhang et al., 2015</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz">download</a></p></td>
<td><p>1,569,264 samples from the Yelp Dataset Challenge 2015. This subset has 280,000 training samples and 19,000 test samples in each polarity.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="image-localization">
<h2><a class="reference external" href="https://registry.opendata.aws/fast-ai-imagelocal/">Image localization</a><a class="headerlink" href="#image-localization" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>Citation</p></th>
<th class="head"><p>Download</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/">Camvid: Motion-based Segmentation and Recognition Dataset</a></p></td>
<td><p><a class="reference external" href="https://pdfs.semanticscholar.org/08f6/24f7ee5c3b05b1b604357fb1532241e208db.pdf">Brostow et al., 2008</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imagelocal/camvid.tgz">download</a></p></td>
<td><p>Segmentation dataset with per-pixel semantic segmentation of over 700 images, each inspected and confirmed by a second person for accuracy.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL Visual Object Classes (VOC)</a></p></td>
<td><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf">Everingham, M et al., 2010</a></p></td>
<td><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-imagelocal/pascal-voc.tgz">download</a></p></td>
<td><p>Standardised image data sets for object class recognition - both 2007 and 2012 versions are provided here. The 2012 version has 20 classes. The train/val data has 11,530 images containing 27,450 ROI annotated objects and 6,929 segmentations. There are also simplifed version for the annotated objects of the <a class="reference external" href="https://s3.amazonaws.com/fast-ai-imagelocal/pascal_2007.tgz">2007 version</a> and the <a class="reference external" href="https://s3.amazonaws.com/fast-ai-imagelocal/pascal_2012.tgz">2012 version</a>.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="coco">
<h3><a class="reference external" href="https://registry.opendata.aws/fast-ai-coco/">COCO</a><a class="headerlink" href="#coco" title="Permalink to this headline">¶</a></h3>
<p>Probably the most widely used dataset today for object localization is <a class="reference external" href="https://arxiv.org/abs/1405.0312">COCO: Common Objects in Context</a>. Provided here are all the files from the 2017 version, along with an additional <em>subset</em> dataset created by fast.ai. Details of each COCO dataset is available from the <a class="reference external" href="http://cocodataset.org/#download">COCO dataset page</a>. The fast.ai subset contains all images that contain one of five selected categories, restricting objects to just those five categories; the categories are: chair couch tv remote book vase.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/coco_sample.tgz">fast.ai subset</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/train2017.zip">Train images</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/val2017.zip">Val images</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/test2017.zip">Test images</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/unlabeled2017.zip">Unlabeled images</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/image_info_test2017.zip">Testing Image info</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/image_info_unlabeled2017.zip">Unlabeled Image info</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/annotations_trainval2017.zip">Train/Val annotations</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/stuff_annotations_trainval2017.zip">Stuff Train/Val annotations</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/fast-ai-coco/panoptic_annotations_trainval2017.zip">Panoptic Train/Val annotations</a></p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/computer-vision"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">Computer Vision</a>
    <a class='right-next' id="next-link" href="datasets.html" title="next page">Datasets</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Juan Huerta<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>